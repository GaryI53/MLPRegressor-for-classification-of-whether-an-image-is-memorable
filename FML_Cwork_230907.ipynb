{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877e604c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Import for jupiter notebook styling // Ignore if using colab ##\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6020ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Imports ##\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83be9b4",
   "metadata": {},
   "source": [
    "\n",
    "# Pre-processing\n",
    "\n",
    "This following sections of code are for the pre-processing of data. In doing so I will use KNN classifier to judge whether pre-processing steps are advantageous and fine tune parameters to improve the performance of the training set. The training2 dataset will be split in to a smaller dataset. I will then use KNN imputation to fill in the nan gaps for training2. I will then test this by removing values from training1 and saving the nan filled training1 within a seperate document called training3. We can then use the same KNN imputation model on training3, whilst knowing the training1 ground truth data. I can then measure the distance from original values, calculate the mean percentage of nan values within an image and change the weight of the image accordingly using the confidence rating. Once this has been completed, I can use PCA to reduce dimensionality on both training1 and training2. To choose the correct number of principal components I have the PCA measured in total to 95% variance of the input data. Once PCA is complete, I will combine the two training sets and randomise the image samples to avoid any future bias.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6dda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading training data to variables ##\n",
    "    \n",
    "data=pd.read_csv(\"training1.csv\") \n",
    "dataWithMissing=pd.read_csv(\"training2.csv\")\n",
    "training1WithDataRemoved=pd.read_csv(\"training3.csv\") #this is a copy of training1 with 'n' values removed to test KNNimputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ec159",
   "metadata": {},
   "source": [
    "## Normalisation step\n",
    "Normalise all of the training sets including the training3 which has been created from training1 with missing values. This is used to test imputation as will be explained further in the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8397a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#normalise training1, training2 and training3(manipulated training1 with missing values)\n",
    "\n",
    "dataFeat = data.iloc[:,:4096]\n",
    "dataGistFeat = data.iloc[:,4096:-2] #normalise gist data seperately as will be on seperate scale\n",
    "dataLabel = data.iloc[:,-2:]\n",
    "\n",
    "dataWMFeat = dataWithMissing.iloc[:,:4096]\n",
    "dataGistWMFeat = dataWithMissing.iloc[:,4096:-2]\n",
    "dataWMLabel = dataWithMissing.iloc[:,-2:]\n",
    "\n",
    "dataT1WMFeat = training1WithDataRemoved.iloc[:,:4096]\n",
    "dataGistT1WMFeat = training1WithDataRemoved.iloc[:,4096:-2]\n",
    "dataT1WMLabel = training1WithDataRemoved.iloc[:,-2:]\n",
    "    \n",
    "scaler = MinMaxScaler()\n",
    "Gistscaler = MinMaxScaler()\n",
    "\n",
    "#training data without missing\n",
    "dataFeat = pd.DataFrame(scaler.fit_transform(dataFeat))\n",
    "GistFeat = pd.DataFrame(Gistscaler.fit_transform(dataGistFeat))\n",
    "data = np.column_stack((dataFeat,dataGistFeat,dataLabel))\n",
    "\n",
    "#training2 data with missing\n",
    "dataWMFeat = pd.DataFrame(scaler.transform(dataWMFeat))\n",
    "dataGistWMFeat = pd.DataFrame(Gistscaler.fit_transform(dataGistWMFeat))\n",
    "dataWithMissing = np.column_stack((dataWMFeat,dataGistWMFeat,dataWMLabel))\n",
    "\n",
    "#training1 data with missing\n",
    "dataT1WMFeat = pd.DataFrame(scaler.transform(dataT1WMFeat))\n",
    "dataGistT1WMFeat = pd.DataFrame(Gistscaler.fit_transform(dataGistT1WMFeat))\n",
    "training1WithDataRemoved = np.column_stack((dataT1WMFeat,dataGistT1WMFeat,dataT1WMLabel))\n",
    "\n",
    "my_data_array=data\n",
    "my_dataWithMissing_array=dataWithMissing \n",
    "my_training1WithMissing_array = training1WithDataRemoved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bbad67",
   "metadata": {},
   "source": [
    "## KNN classifier (not part of main classifier or Pre-process)\n",
    "Please note this is not the main classifier, nor does this contribute to any results. This is purely used to test whether the training sets are working as they should during pre-processing stages. Another KNN classifier is also also for the same purposes above to test training2 after imputation. The main classifier from which predictions will be made is the Multi-Layer Perceptron Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4424dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 75.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garyi\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[76, 16],\n",
       "       [28, 60]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## KNN classifier on complete training set 1 ##\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "#seperate training and validation sets of 70% and 30% of data set respectively\n",
    "lengthTraining = len(my_data_array) * 0.70\n",
    "my_data_array_train = my_data_array[:int(lengthTraining)]\n",
    "my_data_array_valid = my_data_array[int(lengthTraining):]\n",
    "\n",
    "#variables for storing manipulated data\n",
    "data_features = []\n",
    "data_labels = []\n",
    "data_features_valid = []\n",
    "data_labels_valid = []\n",
    "Y_train = []\n",
    "X_train = []\n",
    "\n",
    "#seperate the training labels and features\n",
    "for i in my_data_array_train:\n",
    "    data_features.append(i[:-2])\n",
    "    data_labels.append(i[-2:-1])\n",
    "\n",
    "#seperate the validation labels and features\n",
    "for i in my_data_array_valid:\n",
    "    data_features_valid.append(i[:-2])\n",
    "    data_labels_valid.append(i[-2:-1])\n",
    "\n",
    "#add training data to singular list\n",
    "Y_train = np.stack(data_labels)\n",
    "X_train = np.stack(data_features)\n",
    "\n",
    "#add testing data and ground truth labels\n",
    "Y_test_actual = np.stack(data_labels_valid)\n",
    "X_test = np.stack(data_features_valid)\n",
    "\n",
    "#create KNeighbours object,choose amount of neighbours,input the training, validation data - validation results\n",
    "knearest = KNeighborsClassifier(n_neighbors=5)\n",
    "knearest.fit(X_train,Y_train)\n",
    "Y_predict_knearest=knearest.predict(X_test)\n",
    "\n",
    "#simplify label array to singular list to be passed through classifier\n",
    "Y_test_actual = np.squeeze(Y_test_actual)\n",
    "accuracy_knearest = round(accuracy_score(Y_predict_knearest,Y_test_actual)*100,2)\n",
    "print(\"Accuracy score: \"+str(accuracy_knearest)+ \"%\")\n",
    "confusion_matrix(Y_test_actual, Y_predict_knearest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232d18f",
   "metadata": {},
   "source": [
    "## Check Nans\n",
    "This function is used to check for nan values before and after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4a701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for checking whether array contains any nan ##\n",
    "\n",
    "def contains_nan(arrayL):\n",
    "\n",
    "    array_has_nan = np.isnan(arrayL)\n",
    "    hasMissing = False\n",
    "    for i in array_has_nan:\n",
    "        if i.any() == True:\n",
    "            hasMissing = True\n",
    "    return hasMissing    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec7b1c",
   "metadata": {},
   "source": [
    "## KNN Imputer\n",
    "\n",
    "Using the trained KNN imputer, we can fill the nan gaps in training2 with feature data which will is likely to be more accurate than the feature data that can obtained by a simple imputer utilising just mean,mode,median of a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d612b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "## KNN imputer for filling in missing values in array ##\n",
    "\n",
    "#convert data with missing values to a list for use in KNNimputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "dwm = dataWithMissing.tolist()\n",
    "dwm = dwm[:800] #accuracy increased and helps to not negate value of training1\n",
    "dwmLabels = []\n",
    "dwmFeat = []\n",
    "\n",
    "#seperate labels ready for imputation\n",
    "for i in dwm:\n",
    "    dwmFeat.append(i[:-2])\n",
    "    dwmLabels.append(i[-2:])\n",
    "\n",
    "#create imputer object, fit and transform the features \n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "dwmFeat = imputer.fit_transform(dwmFeat)\n",
    "\n",
    "#recombine labels and features\n",
    "dwm = np.column_stack((dwmFeat,dwmLabels)) \n",
    "\n",
    "print(\"done\") #this may take time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4087ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735941\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## proof imputation has removed nan values ##\n",
    "\n",
    "#for use when adjusting confidence score of training2\n",
    "counterMissing = np.count_nonzero(np.isnan(dataWithMissing[:800]))\n",
    "                                \n",
    "print(np.count_nonzero(np.isnan(dataWithMissing[:800])))\n",
    "dwn = pd.DataFrame(dwm)\n",
    "print(np.count_nonzero(np.isnan(dwm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d52571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing accuracy of the KNN imputer Part1 (using created imputer object on training3 which is training1 with missing values) ##\n",
    "\n",
    "#KNN imputer on training 1 (from manipulated training 1 called training 3) with values removed to emulate training 2\n",
    "#By knowing the original values for training 1 we can test our imputers accuracy against ground truth\n",
    "\n",
    "t1dataWithMissing = training1WithDataRemoved.tolist()\n",
    "\n",
    "t1dataWithMissingFeatures = []\n",
    "t1dataWithMissingLabels = []\n",
    "\n",
    "#print(t1dataWithMissing[0])\n",
    "for i in t1dataWithMissing:\n",
    "    t1dataWithMissingFeatures.append(i[:-2])\n",
    "    t1dataWithMissingLabels.append(i[-2:])\n",
    " \n",
    "t1dataWithMissingFeatures = imputer.transform(t1dataWithMissingFeatures)\n",
    "t1dataWithMissing = np.column_stack((t1dataWithMissingFeatures,t1dataWithMissingLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a1a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KNN imputation fills incomplete nan values to approximately 99.19242051295566% percent accuracy of original value\n"
     ]
    }
   ],
   "source": [
    "## testing KNN imputation accuracy Part2 (compare accuracy of imputed nan values in training3 against ground truth for training1) ##\n",
    "\n",
    "data_actual = data.tolist() #holds original training1 data\n",
    "\n",
    "counter = 0      #mean difference between original training1 values and predicted nan\n",
    "totalCounter = 0 #total value of original training1 \n",
    "meanDif = 0\n",
    "counter2 = 0     #total number of nan values\n",
    "\n",
    "#iterates through original training1 with ground truth and training1 with imputed values to test accuracy\n",
    "for x in range(10):\n",
    "        for i in range(len(data_actual)):\n",
    "            if t1dataWithMissing[x][i] != data_actual[x][i]:\n",
    "                counter = counter + (data_actual[x][i] -t1dataWithMissing[x][i]) #keeps total of difference in values\n",
    "                totalCounter = totalCounter + data_actual[x][i] #counts total value from within training1 features where value removed\n",
    "                counter2 = counter2 + 1 #counts amount of nan cells from training3\n",
    "        \n",
    "if (counter < 0):\n",
    "    counter = counter * counter\n",
    "percentageDif = (totalCounter /counter2) / counter\n",
    "percentageDif = 100 - percentageDif\n",
    "\n",
    "print(\"The KNN imputation fills incomplete nan values to approximately \"+str(percentageDif)+ \"% percent accuracy of original value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd9223",
   "metadata": {},
   "source": [
    "# Change confidence ratings according to accuracy\n",
    "The features that have been filled by imputation in training2 are likely to be less accurate than the features in training1 by the nature of not knowing the ground truth of those values for training2. However we have managed to obtain a rough guideline of accuracy from testing the imputer on training1 and the training1 with nans added (training3). We can then measure the percentage of nan values that are contained in training2 and measure the impact of this loss of accuracy. The confidence rating is then adjusted to reflect this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be5be9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change confidence rating for training 2 to reflect innacuracies in imputed missing data ##\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#firstly check proportion of data with is nan so we have a fair reflection of how innacurate whole dataset is\n",
    "counterLen = 0\n",
    "for i in dwm:\n",
    "    for c in i:\n",
    "        counterLen = counterLen +1\n",
    "proportionOfMissing = counterMissing/counterLen\n",
    "\n",
    "#use accuracy of filled nan gaps along with which proportion of data cells which are filled nan gaps\n",
    "percentageDif = percentageDif * proportionOfMissing\n",
    "\n",
    "#update confidence rating of training2         \n",
    "for i in dwm:\n",
    "    percent = (i[-1]/100)*percentageDif\n",
    "    i[-1] = (i[-1] - percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ca83b",
   "metadata": {},
   "source": [
    "This is the second KNN classifier I have used. This is not necessarily part of the pre-processing nor is part of the overall classification. I am just using this to test that training2 is showing trends resembling training1 after imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc0c15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 70.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garyi\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[101,  24],\n",
       "       [ 48,  67]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## KNN classifier on complete training set 2 ##\n",
    "\n",
    "#seperate training and validation sets of 70% and 30% of data set respectively\n",
    "lengthTraining = len(dwm) * 0.70\n",
    "my_training2_train = dwm[:int(lengthTraining)]\n",
    "my_training2_valid = dwm[int(lengthTraining):]\n",
    "\n",
    "#variables for storing manipulated data\n",
    "data_features2 = []\n",
    "data_labels2 = []\n",
    "data_features_valid2 = []\n",
    "data_labels_valid2 = []\n",
    "Y_train2 = []\n",
    "X_train2 = []\n",
    "\n",
    "#seperate the training labels and features\n",
    "for i in my_training2_train:\n",
    "    data_features2.append(i[:-2])\n",
    "    data_labels2.append(i[-2:-1])\n",
    "\n",
    "#seperate the validation labels and features\n",
    "for i in my_training2_valid:\n",
    "    data_features_valid2.append(i[:-2])\n",
    "    data_labels_valid2.append(i[-2:-1])\n",
    "\n",
    "#add training data to singular list\n",
    "Y_train2 = np.stack(data_labels2)\n",
    "X_train2 = np.stack(data_features2)\n",
    "\n",
    "#add testing data and ground truth labels\n",
    "Y_test_actual2 = np.stack(data_labels_valid2)\n",
    "X_test2 = np.stack(data_features_valid2)\n",
    "\n",
    "#create KNeighbours object,choose amount of neighbours,input the training, validation data - validation results\n",
    "knearest2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knearest2.fit(X_train2,Y_train2)\n",
    "Y_predict_knearest2=knearest.predict(X_test2)\n",
    "\n",
    "#simplify label array to singular list to be passed through classifier\n",
    "Y_test_actual2 = np.squeeze(Y_test_actual2)\n",
    "accuracy_knearest2 = round(accuracy_score(Y_predict_knearest2,Y_test_actual2)*100,2)\n",
    "print(\"Accuracy score: \"+str(accuracy_knearest2)+ \"%\")\n",
    "confusion_matrix(Y_test_actual2, Y_predict_knearest2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605ef0c",
   "metadata": {},
   "source": [
    "# PCA (Principal Components Analysis)\n",
    "For both the training1 and training2, we will use PCA to reduce the dimensionality. We will fit the PCA to training1 and then use the PCA transform method to reduce the amount of features in training1 and training2. We will then add the labels to both sets again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "818d9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pca on training sets##\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# placeholders for features, labels and confidence from training set1 and 2 before pca\n",
    "data_feat = []\n",
    "data_lab = []\n",
    "dwm_feat = []\n",
    "dwm_lab = []\n",
    "\n",
    "#seperate the training labels and features\n",
    "for i in my_data_array:\n",
    "        data_feat.append(i[:-2])\n",
    "        data_lab.append(i[-2:])\n",
    "        \n",
    "#seperate the training labels and features\n",
    "for i in dwm:\n",
    "    dwm_feat.append(i[:-2])\n",
    "    dwm_lab.append(i[-2:])\n",
    "    \n",
    "#pca on training set1 \n",
    "pca = PCA(n_components = 0.95) #number of primary components to account for 95% variance\n",
    "pca_data = pca.fit_transform(data_feat)\n",
    "\n",
    "#pca on training set2 (using pca trained on training1 instead of both as more trustworthy having had no nan features)\n",
    "pca_data_withMissing = pca.transform(dwm_feat)\n",
    "\n",
    "# append labels on end of features for both lists \n",
    "train1 = np.column_stack((pca_data,data_lab))\n",
    "train2 = np.column_stack((pca_data_withMissing,dwm_lab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec82db6",
   "metadata": {},
   "source": [
    "# Merge the training sets\n",
    "We will now merge the two training sets as part of the last step of pre-processing. We will also shuffle the two sets in order to avoid any bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79c0d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a single training set from training1 and training2\n",
    "\n",
    "import random\n",
    "\n",
    "#create two lists of training1 and training2 subset, combine and shuffle\n",
    "trainlist1 = train1.tolist()\n",
    "trainlist2 = train2.tolist()\n",
    "trainingData = trainlist1 + trainlist2\n",
    "random.shuffle(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37fbde",
   "metadata": {},
   "source": [
    "# Multilayer perceptron Regressor\n",
    "\n",
    "Using the single dataset containing features,labels and confidence from training1 and imputed training2, the multilayer Perceptron regressor will be trained. Once tuned, we will use the trained multilayer perceptron model on our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf3e8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 375)\n"
     ]
    }
   ],
   "source": [
    "#Convert training and data in to array format\n",
    "\n",
    "testData=pd.read_csv(\"test.csv\") \n",
    "\n",
    "trainingData_array = np.array(trainingData) #test data from both training1 and imputed training2\n",
    "testData_array = np.array(testData) #test data \n",
    "\n",
    "print(trainingData_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd7dd1",
   "metadata": {},
   "source": [
    "## Test data Pre-Processing\n",
    "We will normalise, use imputation and pca on the test data so that it is scaled and in the same format as the training data used to train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a1dfba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values for test features using trained imputer from training data\n",
    "\n",
    "testWithMissing = testData_array.tolist()\n",
    "\n",
    "test = imputer.transform(testWithMissing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07535f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garyi\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Garyi\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#normalise test data using same scaler as training data\n",
    "\n",
    "test1 = test[:,:4096]\n",
    "testGistFeat = test[:,4096:] #the divide between gist and cnn features\n",
    "\n",
    "test1 = pd.DataFrame(scaler.transform(test1)) #Scaling for Gist and CNN data seperately\n",
    "testGistFeat = pd.DataFrame(Gistscaler.transform(testGistFeat))\n",
    "test = np.column_stack((test1,testGistFeat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39f23e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use trained pca on test data to remove same features\n",
    "\n",
    "testInput = pca.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43977f",
   "metadata": {},
   "source": [
    "# Merge confidence and label\n",
    "So that we can use the confidence rating we will then merge the confidence and labels together. On the training data we will change the label to be the same value as the confidence, however positive for memorable and negative for non memorable (for instance -66 will be a 0 label with 66 confidence).  By doing so, this will convert the classifier in to a regression model that once tested against our test data and validation set, will provide a resulting output of any floating point number between -1 to 1. We shall then be able to take a binary result from the variable scales of label data by converting positive labels back to 1 and negative labels back to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aadf49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine confidence and label data \n",
    "\n",
    "#for 1 labels value is positive, for 0 labels value is negative. Allows for binary classification on confidence\n",
    "#convert initially to regression task\n",
    "\n",
    "trainingInputLabels = trainingData_array[:,-2:-1] #seperate labels\n",
    "trainingInputConf = trainingData_array[:,-1:]\n",
    "\n",
    "trainingInputConf = trainingInputConf * 100 \n",
    "\n",
    "for i in range(len(trainingInputLabels)):\n",
    "    if trainingInputLabels[i] > 0:\n",
    "        trainingInputLabels[i] = trainingInputConf[i] #keep positive for label 1\n",
    "    else:\n",
    "        trainingInputLabels[i] = -abs(trainingInputConf[i]) #convert to negative for label 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6eb67a",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Regressor\n",
    "We will now use the MLPRegressor to use the training data which has labels which now scale from -1 to 1 to train the model. By using the regressor, we will use a split of training/validation and check the validation for accuracy. To check for accuracy we will convert back to binary, giving labels 1 for positive intergers and 0 for negative. We can then compare the original binary values, tune the number of hidden layers, random states and learning rate and retrain the MLPR to see if we can improve. Once the accuracy has reached what is deemed a suitable level, this model can be used to classify out test set and produce predictions.. Please be aware that the random seed causes results to differ on re-run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1392b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data in to training/validation sets\n",
    "    \n",
    "trainingData_feat = trainingData_array[:,:-2]\n",
    "lengthTraining = len(trainingData_feat) * 0.70\n",
    "my_train = trainingData_feat[:int(lengthTraining)].tolist()\n",
    "my_valid = trainingData_feat[int(lengthTraining):].tolist()\n",
    "my_train_lab = trainingInputLabels[:int(lengthTraining)].tolist()\n",
    "my_valid_actual = trainingInputLabels[int(lengthTraining):].tolist()\n",
    "Y_pred_lab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad184f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garyi\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1599: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2217.62560297\n",
      "Iteration 2, loss = 1498.01580754\n",
      "Iteration 3, loss = 1119.73904895\n",
      "Iteration 4, loss = 840.58768593\n",
      "Iteration 5, loss = 618.01338998\n",
      "Iteration 6, loss = 411.38667735\n",
      "Iteration 7, loss = 251.60640609\n",
      "Iteration 8, loss = 150.20324010\n",
      "Iteration 9, loss = 88.52587942\n",
      "Iteration 10, loss = 81.46123204\n",
      "Iteration 11, loss = 68.47529417\n",
      "Iteration 12, loss = 60.51701818\n",
      "Iteration 13, loss = 44.20719142\n",
      "Iteration 14, loss = 41.32096885\n",
      "Iteration 15, loss = 27.07836175\n",
      "Iteration 16, loss = 25.36606703\n",
      "Iteration 17, loss = 15.11928019\n",
      "Iteration 18, loss = 11.95400664\n",
      "Iteration 19, loss = 10.49452227\n",
      "Iteration 20, loss = 8.50216478\n",
      "Iteration 21, loss = 7.10527079\n",
      "Iteration 22, loss = 4.85886125\n",
      "Iteration 23, loss = 4.07552114\n",
      "Iteration 24, loss = 3.38279970\n",
      "Iteration 25, loss = 2.51887649\n",
      "Iteration 26, loss = 1.92241756\n",
      "Iteration 27, loss = 1.20574042\n",
      "Iteration 28, loss = 0.92979502\n",
      "Iteration 29, loss = 0.56464595\n",
      "Iteration 30, loss = 0.42811048\n",
      "Iteration 31, loss = 0.29096084\n",
      "Iteration 32, loss = 0.23391971\n",
      "Iteration 33, loss = 0.14814271\n",
      "Iteration 34, loss = 0.09640421\n",
      "Iteration 35, loss = 0.08630936\n",
      "Iteration 36, loss = 0.05923113\n",
      "Iteration 37, loss = 0.04500177\n",
      "Iteration 38, loss = 0.03333447\n",
      "Iteration 39, loss = 0.02674379\n",
      "Iteration 40, loss = 0.01641732\n",
      "Iteration 41, loss = 0.01160551\n",
      "Iteration 42, loss = 0.00946315\n",
      "Iteration 43, loss = 0.00800471\n",
      "Iteration 44, loss = 0.00564912\n",
      "Iteration 45, loss = 0.00460653\n",
      "Iteration 46, loss = 0.00461728\n",
      "Iteration 47, loss = 0.00383275\n",
      "Iteration 48, loss = 0.00360363\n",
      "Iteration 49, loss = 0.00263565\n",
      "Iteration 50, loss = 0.00205932\n",
      "Iteration 51, loss = 0.00181780\n",
      "Iteration 52, loss = 0.00138731\n",
      "Iteration 53, loss = 0.00119963\n",
      "Iteration 54, loss = 0.00128730\n",
      "Iteration 55, loss = 0.00125887\n",
      "Iteration 56, loss = 0.00118410\n",
      "Iteration 57, loss = 0.00122804\n",
      "Iteration 58, loss = 0.00107077\n",
      "Iteration 59, loss = 0.00091781\n",
      "Iteration 60, loss = 0.00083758\n",
      "Iteration 61, loss = 0.00082119\n",
      "Iteration 62, loss = 0.00074701\n",
      "Iteration 63, loss = 0.00072743\n",
      "Iteration 64, loss = 0.00069524\n",
      "Iteration 65, loss = 0.00066010\n",
      "Iteration 66, loss = 0.00066202\n",
      "Iteration 67, loss = 0.00065337\n",
      "Iteration 68, loss = 0.00068538\n",
      "Iteration 69, loss = 0.00064580\n",
      "Iteration 70, loss = 0.00061496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(300, 100, 200, 300), learning_rate_init=0.01,\n",
       "             random_state=5, verbose=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "MLPR = MLPRegressor(hidden_layer_sizes=(300,100,200,300),\n",
    "                    random_state=5,\n",
    "                    verbose=True,\n",
    "                    learning_rate_init=0.01,\n",
    "                    #activation='tanh',\n",
    "                    #batch_size = 23\n",
    "                  )\n",
    "\n",
    "MLPR.fit(my_train,my_train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eefa2a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7054631828978623"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "Y_pred_labels=MLPR.predict(my_valid)\n",
    "\n",
    "#convert regression values to binary classification (<0 = non-memorable >0 = memorable)\n",
    "\n",
    "#convert predicted labels\n",
    "for i in range(len(Y_pred_labels)):\n",
    "    if Y_pred_labels[i] > 0:\n",
    "        Y_pred_labels[i] = 1\n",
    "    else:\n",
    "        Y_pred_labels[i] = 0\n",
    "        \n",
    "my_valid_actual = np.array(my_valid_actual)\n",
    "\n",
    "#convert actual labels\n",
    "for i in range(len(my_valid_actual)):\n",
    "    if my_valid_actual[i] > 0:\n",
    "        my_valid_actual[i] = 1\n",
    "    else:\n",
    "        my_valid_actual[i] = 0\n",
    "    \n",
    "accuracy_score(Y_pred_labels, my_valid_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcd9473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#create predictions on test set\n",
    "\n",
    "Y_pred_labels=MLPR.predict(testInput)\n",
    "\n",
    "for i in range(len(Y_pred_labels)):\n",
    "    if Y_pred_labels[i] > 0:\n",
    "        Y_pred_labels[i] = 1\n",
    "    else:\n",
    "        Y_pred_labels[i] = 0\n",
    "\n",
    "for i in Y_pred_labels:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0435f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
